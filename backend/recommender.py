import os
import sys
import tempfile
import numpy as np
from pymongo import MongoClient
from bson.objectid import ObjectId  # <--- Î— Î£Î—ÎœÎ‘ÎÎ¤Î™ÎšÎ— Î”Î™ÎŸÎ¡Î˜Î©Î£Î— Î•Î”Î©
from pyspark.sql import SparkSession
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Normalizer

# --- Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ Î“Î™Î‘ WINDOWS & JAVA 17 ---


JAVA_HOME_PATH = r"C:\Program Files\Microsoft\jdk-17.0.17.10-hotspot" 


import ctypes
def get_short_path_name(long_name):
    output_buf_size = 0
    while True:
        output_buf_size += 1024
        output_buf = ctypes.create_unicode_buffer(output_buf_size)
        needed = ctypes.windll.kernel32.GetShortPathNameW(long_name, output_buf, output_buf_size)
        if output_buf_size >= needed:
            return output_buf.value

try:
    if os.path.exists(JAVA_HOME_PATH):
        short_java_home = get_short_path_name(JAVA_HOME_PATH)
        os.environ["JAVA_HOME"] = short_java_home
        print(f" Î¡Ï…Î¸Î¼Î¯ÏƒÏ„Î·ÎºÎµ Ï„Î¿ JAVA_HOME ÏƒÎµ: {short_java_home}")
    else:
        print(f" Î Î¡ÎŸÎ£ÎŸÎ§Î—: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Î¿ Ï†Î¬ÎºÎµÎ»Î¿Ï‚: {JAVA_HOME_PATH}")
        print("   Î Î±ÏÎ±ÎºÎ±Î»Ï Î´Î¹ÏŒÏÎ¸Ï‰ÏƒÎµ Ï„Î· Î³ÏÎ±Î¼Î¼Î® 14 ÏƒÏ„Î¿ script!")
except Exception as e:
    print(f"Warning: Î”ÎµÎ½ Î¼Ï€Î¿ÏÎ­ÏƒÎ±Î¼Îµ Î½Î± Ï†Ï„Î¹Î¬Î¾Î¿Ï…Î¼Îµ short path. {e}")
    os.environ["JAVA_HOME"] = JAVA_HOME_PATH

os.environ["PYTHONIOENCODING"] = "utf-8"
os.environ["PYSPARK_PYTHON"] = sys.executable
os.environ["PYSPARK_DRIVER_PYTHON"] = sys.executable

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€ÏÎ¿ÏƒÏ‰ÏÎ¹Î½Î¿Ï Ï†Î±ÎºÎ­Î»Î¿Ï… Î³Î¹Î± Hadoop
temp_dir = tempfile.mkdtemp()
os.environ["HADOOP_HOME"] = temp_dir

def calculate_recommendations():
    print(" ÎÎµÎºÎ¹Î½Î¬ÎµÎ¹ Ï„Î¿ Spark Recommendation Job...")

    # Î£ÏÎ½Î´ÎµÏƒÎ· Î¼Îµ MongoDB
    try:
        client = MongoClient("mongodb://127.0.0.1:27017/")
        db = client["coursesDB"]
        collection = db["courses"]
        if collection.count_documents({}) == 0:
            print(" Î— Î²Î¬ÏƒÎ· ÎµÎ¯Î½Î±Î¹ Î¬Î´ÎµÎ¹Î±! Î¤ÏÎ­Î¾Îµ Ï€ÏÏÏ„Î± Ï„Î¿ 'node importData.js'.")
            return
    except Exception as e:
        print(f" Î£Ï†Î¬Î»Î¼Î± Mongo: {e}")
        return

    print(" Î‘Î½Î¬Î³Î½Ï‰ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î±Ï€ÏŒ MongoDB...")
    cursor = collection.find({}, {"_id": 1, "title": 1, "description": 1, "keywords": 1})
    data = []
    for doc in cursor:
        keywords = " ".join(doc.get("keywords", [])) if doc.get("keywords") else ""
        text = f"{doc.get('title','')} {doc.get('description','')} {keywords}".lower()
        data.append((str(doc["_id"]), text))

    # Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Spark
    print(" Î•ÎºÎºÎ¯Î½Î·ÏƒÎ· Spark Session...")
    try:
        spark = SparkSession.builder \
            .appName("CourseRecommender") \
            .master("local[*]") \
            .config("spark.driver.host", "127.0.0.1") \
            .config("spark.driver.bindAddress", "127.0.0.1") \
            .config("spark.ui.enabled", "false") \
            .config("spark.driver.extraJavaOptions", "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED") \
            .getOrCreate()
        spark.sparkContext.setLogLevel("ERROR")
    except Exception as e:
        print("âŒ Î£Ï†Î¬Î»Î¼Î± Spark.")
        print(e)
        return

    # ML Pipeline
    df = spark.createDataFrame(data, ["id", "text"])
    print(f" Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± {df.count()} Î¼Î±Î¸Î·Î¼Î¬Ï„Ï‰Î½...")

    tokenizer = Tokenizer(inputCol="text", outputCol="words")
    remover = StopWordsRemover(inputCol="words", outputCol="filtered")
    hashingTF = HashingTF(inputCol="filtered", outputCol="rawFeatures", numFeatures=5000)
    idf = IDF(inputCol="rawFeatures", outputCol="features")
    normalizer = Normalizer(inputCol="features", outputCol="normFeatures")

    pipeline_data = normalizer.transform(
        idf.fit(hashingTF.transform(remover.transform(tokenizer.transform(df))))
        .transform(hashingTF.transform(remover.transform(tokenizer.transform(df))))
    )

    print(" Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Cosine Similarity...")
    rows = pipeline_data.select("id", "normFeatures").collect()
    ids = [row['id'] for row in rows]
    vectors = [row['normFeatures'].toArray() for row in rows]
    matrix = np.array(vectors)
    similarity_matrix = np.dot(matrix, matrix.T)

    print(" Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ· Î²Î¬ÏƒÎ·Ï‚...")
    updates = 0
    for i, course_id in enumerate(ids):
        scores = similarity_matrix[i]
        top_indices = np.argsort(scores)[::-1][1:6]
        related_ids = [ids[idx] for idx in top_indices]
        
        # --- Î— Î”Î™ÎŸÎ¡Î˜Î©Î£Î— Î•Î™ÎÎ‘Î™ Î•Î”Î© ---
        collection.update_one(
            {"_id": ObjectId(course_id)}, # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿ ObjectId Ï€Î¿Ï… ÎºÎ¬Î½Î±Î¼Îµ import
            {"$set": {"relatedCourseIds": related_ids}}
        )
        updates += 1
        if updates % 1000 == 0: print(f"   ... {updates} done")

    print(f"ğŸ‰ Î¤Î•Î›ÎŸÎ£! Î•Î½Î·Î¼ÎµÏÏÎ¸Î·ÎºÎ±Î½ {updates} Î¼Î±Î¸Î®Î¼Î±Ï„Î± Î¼Îµ recommendations.")
    spark.stop()

if __name__ == "__main__":
    calculate_recommendations()