import os
import sys
import tempfile
from pymongo import MongoClient
from bson.objectid import ObjectId
from pyspark.sql import SparkSession
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Normalizer
from pyspark.ml.clustering import KMeans

# --- Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ Î“Î™Î‘ WINDOWS & JAVA 17 ---


JAVA_HOME_PATH = r"C:\Program Files\Eclipse Adoptium\jdk-17.0.14.7-hotspot"



# 2. Î¤ÏÎ¹Îº Î³Î¹Î± Î½Î± Î¼Î·Î½ ÎºÎ¿Î»Î»Î¬ÎµÎ¹ ÏƒÏ„Î± ÎºÎµÎ½Î¬ Ï„Î¿Ï… "Program Files"
import ctypes
def get_short_path_name(long_name):
    output_buf_size = 0
    while True:
        output_buf_size += 1024
        output_buf = ctypes.create_unicode_buffer(output_buf_size)
        needed = ctypes.windll.kernel32.GetShortPathNameW(long_name, output_buf, output_buf_size)
        if output_buf_size >= needed:
            return output_buf.value

try:
    if os.path.exists(JAVA_HOME_PATH):
        short_java_home = get_short_path_name(JAVA_HOME_PATH)
        os.environ["JAVA_HOME"] = short_java_home
        print(f" Î¡Ï…Î¸Î¼Î¯ÏƒÏ„Î·ÎºÎµ Ï„Î¿ JAVA_HOME ÏƒÎµ: {short_java_home}")
    else:
        print(f" Î Î¡ÎŸÎ£ÎŸÎ§Î—: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Î¿ Ï†Î¬ÎºÎµÎ»Î¿Ï‚: {JAVA_HOME_PATH}")
        print("   Î Î±ÏÎ±ÎºÎ±Î»Ï Î´Î¹ÏŒÏÎ¸Ï‰ÏƒÎµ Ï„Î· Î³ÏÎ±Î¼Î¼Î® 14 ÏƒÏ„Î¿ script!")
except Exception as e:
    print(f"Warning: Î”ÎµÎ½ Î¼Ï€Î¿ÏÎ­ÏƒÎ±Î¼Îµ Î½Î± Ï†Ï„Î¹Î¬Î¾Î¿Ï…Î¼Îµ short path. {e}")
    os.environ["JAVA_HOME"] = JAVA_HOME_PATH

os.environ["PYTHONIOENCODING"] = "utf-8"
os.environ["PYSPARK_PYTHON"] = sys.executable
os.environ["PYSPARK_DRIVER_PYTHON"] = sys.executable

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€ÏÎ¿ÏƒÏ‰ÏÎ¹Î½Î¿Ï Ï†Î±ÎºÎ­Î»Î¿Ï… Î³Î¹Î± Hadoop
temp_dir = tempfile.mkdtemp()
os.environ["HADOOP_HOME"] = temp_dir

def perform_clustering(num_clusters=15, seed=42):
    """
    ÎŸÎ¼Î±Î´Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î¼Î±Î¸Î·Î¼Î¬Ï„Ï‰Î½ ÏƒÎµ clusters Î²Î¬ÏƒÎµÎ¹ Î¸ÎµÎ¼Î±Ï„Î¹ÎºÎ¿Ï Ï€ÎµÏÎ¹ÎµÏ‡Î¿Î¼Î­Î½Î¿Ï….
    
    Args:
        num_clusters: Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ clusters (default: 15)
                     - Î“Î¹Î± 5 ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚: 15 clusters = ~3 sub-clusters Î±Î½Î¬ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±
                     - Î“Î¹Î± 7200 Î¼Î±Î¸Î®Î¼Î±Ï„Î±: 15-20 clusters ÎµÎ¯Î½Î±Î¹ ÎºÎ±Î»Î® Î±ÏÏ‡Î®
        seed: Random seed Î³Î¹Î± Î±Î½Î±Ï€Î±ÏÎ±Î³Ï‰Î³Î¹Î¼ÏŒÏ„Î·Ï„Î± (default: 42)
    """
    print(" ÎÎµÎºÎ¹Î½Î¬ÎµÎ¹ Ï„Î¿ Spark Clustering Job...")
    print(f" Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ clusters: {num_clusters}")

    # Î£ÏÎ½Î´ÎµÏƒÎ· Î¼Îµ MongoDB
    try:
        client = MongoClient("mongodb://127.0.0.1:27017/")
        db = client["coursesDB"]
        collection = db["courses"]
        if collection.count_documents({}) == 0:
            print(" Î— Î²Î¬ÏƒÎ· ÎµÎ¯Î½Î±Î¹ Î¬Î´ÎµÎ¹Î±! Î¤ÏÎ­Î¾Îµ Ï€ÏÏÏ„Î± Ï„Î¿ 'node importData.js'.")
            return
    except Exception as e:
        print(f" Î£Ï†Î¬Î»Î¼Î± Mongo: {e}")
        return

    print(" Î‘Î½Î¬Î³Î½Ï‰ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î±Ï€ÏŒ MongoDB...")
    cursor = collection.find({}, {"_id": 1, "title": 1, "description": 1, "keywords": 1})
    data = []
    for doc in cursor:
        keywords = " ".join(doc.get("keywords", [])) if doc.get("keywords") else ""
        text = f"{doc.get('title','')} {doc.get('description','')} {keywords}".lower()
        data.append((str(doc["_id"]), text))

    # Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Spark
    print(" Î•ÎºÎºÎ¯Î½Î·ÏƒÎ· Spark Session...")
    try:
        spark = SparkSession.builder \
            .appName("CourseClustering") \
            .master("local[*]") \
            .config("spark.driver.host", "127.0.0.1") \
            .config("spark.driver.bindAddress", "127.0.0.1") \
            .config("spark.ui.enabled", "false") \
            .config("spark.driver.extraJavaOptions", "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED") \
            .getOrCreate()
        spark.sparkContext.setLogLevel("ERROR")
    except Exception as e:
        print("âŒ Î£Ï†Î¬Î»Î¼Î± Spark.")
        print(e)
        return

    # ML Pipeline Î³Î¹Î± feature extraction
    df = spark.createDataFrame(data, ["id", "text"])
    print(f" Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± {df.count()} Î¼Î±Î¸Î·Î¼Î¬Ï„Ï‰Î½...")

    tokenizer = Tokenizer(inputCol="text", outputCol="words")
    remover = StopWordsRemover(inputCol="words", outputCol="filtered")
    hashingTF = HashingTF(inputCol="filtered", outputCol="rawFeatures", numFeatures=5000)
    idf = IDF(inputCol="rawFeatures", outputCol="features")
    normalizer = Normalizer(inputCol="features", outputCol="normFeatures")

    # Feature extraction pipeline
    pipeline_data = normalizer.transform(
        idf.fit(hashingTF.transform(remover.transform(tokenizer.transform(df))))
        .transform(hashingTF.transform(remover.transform(tokenizer.transform(df))))
    )

    print(" Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· KMeans clustering model...")
    # KMeans Clustering
    kmeans = KMeans(
        featuresCol="normFeatures",
        predictionCol="clusterId",
        k=num_clusters,
        seed=seed,
        maxIter=20
    )
    
    model = kmeans.fit(pipeline_data)
    
    # Î ÏÎ¿Î²Î»Î­Ï€Î¿Ï…Î¼Îµ Ï„Î± clusters Î³Î¹Î± ÎºÎ¬Î¸Îµ Î¼Î¬Î¸Î·Î¼Î±
    clustered_data = model.transform(pipeline_data)
    
    print(" Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ· Î²Î¬ÏƒÎ·Ï‚ Î¼Îµ cluster IDs...")
    # Î£Ï…Î»Î»Î¿Î³Î® Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    results = clustered_data.select("id", "clusterId").collect()
    
    updates = 0
    for row in results:
        course_id = row['id']
        cluster_id = int(row['clusterId'])  # ÎœÎµÏ„Î±Ï„ÏÎ­Ï€Î¿Ï…Î¼Îµ ÏƒÎµ int
        
        # Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ· Ï„Î¿Ï… Î¼Î±Î¸Î®Î¼Î±Ï„Î¿Ï‚ Î¼Îµ Ï„Î¿ clusterId
        collection.update_one(
            {"_id": ObjectId(course_id)},
            {"$set": {"clusterId": cluster_id}}
        )
        updates += 1
        if updates % 100 == 0:
            print(f"   ... {updates} Î¼Î±Î¸Î®Î¼Î±Ï„Î± ÎµÎ½Î·Î¼ÎµÏÏÎ¸Î·ÎºÎ±Î½")

    # Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½ clusters
    cluster_counts = {}
    for row in results:
        cluster_id = int(row['clusterId'])
        cluster_counts[cluster_id] = cluster_counts.get(cluster_id, 0) + 1

    print(f"\nğŸ“Š Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Clusters:")
    for cluster_id in sorted(cluster_counts.keys()):
        print(f"   Cluster {cluster_id}: {cluster_counts[cluster_id]} Î¼Î±Î¸Î®Î¼Î±Ï„Î±")

    # Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ WSSSE (Within Set Sum of Squared Errors)
    wssse = model.summary.trainingCost
    print(f"\nğŸ“ˆ WSSSE (Within Set Sum of Squared Errors): {wssse:.2f}")

    print(f"\nğŸ‰ Î¤Î•Î›ÎŸÎ£! Î•Î½Î·Î¼ÎµÏÏÎ¸Î·ÎºÎ±Î½ {updates} Î¼Î±Î¸Î®Î¼Î±Ï„Î± Î¼Îµ cluster IDs.")
    spark.stop()
    return wssse

def find_optimal_clusters(min_k=5, max_k=30, step=2, seed=42):
    """
    Elbow Method: Î’ÏÎ¯ÏƒÎºÎµÎ¹ Ï„Î¿Î½ Î²Î­Î»Ï„Î¹ÏƒÏ„Î¿ Î±ÏÎ¹Î¸Î¼ÏŒ clusters Î´Î¿ÎºÎ¹Î¼Î¬Î¶Î¿Î½Ï„Î±Ï‚ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Ï„Î¹Î¼Î­Ï‚.
    
    Args:
        min_k: Î•Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Ï‚ Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ clusters (default: 5)
        max_k: ÎœÎ­Î³Î¹ÏƒÏ„Î¿Ï‚ Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ clusters (default: 30)
        step: Î’Î®Î¼Î± Î¼ÎµÏ„Î±Î¾Ï Ï„Ï‰Î½ Ï„Î¹Î¼ÏÎ½ (default: 2)
        seed: Random seed Î³Î¹Î± Î±Î½Î±Ï€Î±ÏÎ±Î³Ï‰Î³Î¹Î¼ÏŒÏ„Î·Ï„Î± (default: 42)
    """
    print("=" * 60)
    print(" ğŸ” ELBOW METHOD: Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î²Î­Î»Ï„Î¹ÏƒÏ„Î¿Ï… Î±ÏÎ¹Î¸Î¼Î¿Ï clusters")
    print("=" * 60)
    
    # Î£ÏÎ½Î´ÎµÏƒÎ· Î¼Îµ MongoDB
    try:
        client = MongoClient("mongodb://127.0.0.1:27017/")
        db = client["coursesDB"]
        collection = db["courses"]
        if collection.count_documents({}) == 0:
            print(" Î— Î²Î¬ÏƒÎ· ÎµÎ¯Î½Î±Î¹ Î¬Î´ÎµÎ¹Î±! Î¤ÏÎ­Î¾Îµ Ï€ÏÏÏ„Î± Ï„Î¿ 'node importData.js'.")
            return
    except Exception as e:
        print(f" Î£Ï†Î¬Î»Î¼Î± Mongo: {e}")
        return

    print(" Î‘Î½Î¬Î³Î½Ï‰ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î±Ï€ÏŒ MongoDB...")
    cursor = collection.find({}, {"_id": 1, "title": 1, "description": 1, "keywords": 1})
    data = []
    for doc in cursor:
        keywords = " ".join(doc.get("keywords", [])) if doc.get("keywords") else ""
        text = f"{doc.get('title','')} {doc.get('description','')} {keywords}".lower()
        data.append((str(doc["_id"]), text))

    # Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Spark
    print(" Î•ÎºÎºÎ¯Î½Î·ÏƒÎ· Spark Session...")
    try:
        spark = SparkSession.builder \
            .appName("CourseClusteringElbow") \
            .master("local[*]") \
            .config("spark.driver.host", "127.0.0.1") \
            .config("spark.driver.bindAddress", "127.0.0.1") \
            .config("spark.ui.enabled", "false") \
            .config("spark.driver.extraJavaOptions", "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED") \
            .getOrCreate()
        spark.sparkContext.setLogLevel("ERROR")
    except Exception as e:
        print("âŒ Î£Ï†Î¬Î»Î¼Î± Spark.")
        print(e)
        return

    # ML Pipeline Î³Î¹Î± feature extraction (Î¼Î¯Î± Ï†Î¿ÏÎ¬)
    df = spark.createDataFrame(data, ["id", "text"])
    print(f" Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± {df.count()} Î¼Î±Î¸Î·Î¼Î¬Ï„Ï‰Î½...")

    tokenizer = Tokenizer(inputCol="text", outputCol="words")
    remover = StopWordsRemover(inputCol="words", outputCol="filtered")
    hashingTF = HashingTF(inputCol="filtered", outputCol="rawFeatures", numFeatures=5000)
    idf = IDF(inputCol="rawFeatures", outputCol="features")
    normalizer = Normalizer(inputCol="features", outputCol="normFeatures")

    # Feature extraction pipeline (Î¼Î¯Î± Ï†Î¿ÏÎ¬)
    pipeline_data = normalizer.transform(
        idf.fit(hashingTF.transform(remover.transform(tokenizer.transform(df))))
        .transform(hashingTF.transform(remover.transform(tokenizer.transform(df))))
    )

    # Î”Î¿ÎºÎ¹Î¼Î® Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ Î±ÏÎ¹Î¸Î¼ÏÎ½ clusters
    results = []
    k_values = list(range(min_k, max_k + 1, step))
    
    print(f"\nğŸ“Š Î”Î¿ÎºÎ¹Î¼Î® {len(k_values)} Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ Î±ÏÎ¹Î¸Î¼ÏÎ½ clusters: {k_values}")
    print("-" * 60)
    
    for k in k_values:
        print(f" ğŸ”„ Î”Î¿ÎºÎ¹Î¼Î® Î¼Îµ {k} clusters...", end=" ")
        try:
            kmeans = KMeans(
                featuresCol="normFeatures",
                predictionCol="clusterId",
                k=k,
                seed=seed,
                maxIter=20
            )
            model = kmeans.fit(pipeline_data)
            wssse = model.summary.trainingCost
            results.append((k, wssse))
            print(f"WSSSE: {wssse:.2f}")
        except Exception as e:
            print(f"Î£Ï†Î¬Î»Î¼Î±: {e}")
            continue

    spark.stop()

    if not results:
        print("âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±!")
        return

    # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    print("\n" + "=" * 60)
    print(" ğŸ“ˆ Î‘Î ÎŸÎ¤Î•Î›Î•Î£ÎœÎ‘Î¤Î‘ ELBOW METHOD:")
    print("=" * 60)
    print(f"{'Clusters':<12} {'WSSSE':<15} {'Î”Î¹Î±Ï†Î¿ÏÎ¬':<15} {'% ÎœÎµÎ¯Ï‰ÏƒÎ·':<15}")
    print("-" * 60)
    
    for i, (k, wssse) in enumerate(results):
        if i == 0:
            diff = 0
            pct_reduction = 0
        else:
            diff = results[i-1][1] - wssse
            pct_reduction = (diff / results[i-1][1]) * 100 if results[i-1][1] > 0 else 0
        print(f"{k:<12} {wssse:<15.2f} {diff:<15.2f} {pct_reduction:<15.2f}%")

    # Î•ÏÏÎµÏƒÎ· Ï„Î¿Ï… "elbow point" (ÏƒÎ·Î¼ÎµÎ¯Î¿ ÏŒÏ€Î¿Ï… Î· Î¼ÎµÎ¯Ï‰ÏƒÎ· Ï„Î¿Ï… WSSSE Î±ÏÏ‡Î¯Î¶ÎµÎ¹ Î½Î± ÎµÏ€Î¹Î²ÏÎ±Î´ÏÎ½ÎµÏ„Î±Î¹)
    print("\n" + "=" * 60)
    print(" ğŸ’¡ Î£Î¥ÎœÎ’ÎŸÎ¥Î›Î•Î£:")
    print("=" * 60)
    
    if len(results) >= 2:
        # Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ rate of change
        reductions = []
        for i in range(1, len(results)):
            prev_wssse = results[i-1][1]
            curr_wssse = results[i][1]
            reduction = prev_wssse - curr_wssse
            reductions.append((results[i][0], reduction))
        
        # Î’ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ Ï„Î¿ ÏƒÎ·Î¼ÎµÎ¯Î¿ ÏŒÏ€Î¿Ï… Î· Î¼ÎµÎ¯Ï‰ÏƒÎ· Î±ÏÏ‡Î¯Î¶ÎµÎ¹ Î½Î± Î¼ÎµÎ¹ÏÎ½ÎµÏ„Î±Î¹ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ¬
        if len(reductions) >= 2:
            reduction_rates = []
            for i in range(1, len(reductions)):
                prev_reduction = reductions[i-1][1]
                curr_reduction = reductions[i][1]
                if prev_reduction > 0:
                    rate_change = ((prev_reduction - curr_reduction) / prev_reduction) * 100
                    reduction_rates.append((reductions[i][0], rate_change))
            
            # Î¤Î¿ elbow point ÎµÎ¯Î½Î±Î¹ ÏŒÏ€Î¿Ï… Î· rate of change ÎµÎ¯Î½Î±Î¹ Î¼ÎµÎ³Î¬Î»Î· (ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ ÏŒÏ„Î¹ Î· Î¼ÎµÎ¯Ï‰ÏƒÎ· ÎµÏ€Î¹Î²ÏÎ±Î´ÏÎ½ÎµÏ„Î±Î¹)
            if reduction_rates:
                # Î’ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ Ï„Î¿ ÏƒÎ·Î¼ÎµÎ¯Î¿ Î¼Îµ Ï„Î· Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ· ÎµÏ€Î¹Î²ÏÎ¬Î´Ï…Î½ÏƒÎ·
                elbow_point = max(reduction_rates, key=lambda x: x[1])
                print(f" ğŸ¯ Î ÏÎ¿Ï„ÎµÎ¹Î½ÏŒÎ¼ÎµÎ½Î¿Ï‚ Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ clusters: {elbow_point[0]}")
                print(f"    (Î£Î·Î¼ÎµÎ¯Î¿ ÏŒÏ€Î¿Ï… Î· Î¼ÎµÎ¯Ï‰ÏƒÎ· WSSSE ÎµÏ€Î¹Î²ÏÎ±Î´ÏÎ½ÎµÏ„Î±Î¹ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ¬)")
        
        # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Ï‰Î½ 3 ÎºÎ±Î»ÏÏ„ÎµÏÏ‰Î½ ÎµÏ€Î¹Î»Î¿Î³ÏÎ½
        print(f"\n ğŸ“Š Top 3 ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚ (Î²Î¬ÏƒÎµÎ¹ WSSSE):")
        sorted_results = sorted(results, key=lambda x: x[1])
        for i, (k, wssse) in enumerate(sorted_results[:3], 1):
            print(f"    {i}. {k} clusters - WSSSE: {wssse:.2f}")
    
    print(f"\n ğŸ’¡ Î“Î¹Î± Î½Î± Ï„ÏÎ­Î¾ÎµÎ¹Ï‚ clustering Î¼Îµ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿ Î±ÏÎ¹Î¸Î¼ÏŒ:")
    print(f"    python clustering.py [Î±ÏÎ¹Î¸Î¼ÏŒÏ‚_clusters]")
    print("=" * 60)

if __name__ == "__main__":
    # Î•Î»Î­Î³Ï‡Î¿Ï…Î¼Îµ Î±Î½ Î¸Î­Î»ÎµÎ¹ Î½Î± Ï„ÏÎ­Î¾ÎµÎ¹ Elbow Method
    if len(sys.argv) > 1 and sys.argv[1] == "--elbow":
        # Elbow Method: Î²ÏÎ¯ÏƒÎºÎµÎ¹ Ï„Î¿Î½ Î²Î­Î»Ï„Î¹ÏƒÏ„Î¿ Î±ÏÎ¹Î¸Î¼ÏŒ clusters
        min_k = int(sys.argv[2]) if len(sys.argv) > 2 else 5
        max_k = int(sys.argv[3]) if len(sys.argv) > 3 else 30
        step = int(sys.argv[4]) if len(sys.argv) > 4 else 2
        find_optimal_clusters(min_k=min_k, max_k=max_k, step=step)
    else:
        
        num_clusters = int(sys.argv[1]) if len(sys.argv) > 1 else 15
        perform_clustering(num_clusters=num_clusters)

